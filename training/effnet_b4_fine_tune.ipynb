{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sought-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision import transforms\n",
    "from torchvision.models import vgg11\n",
    "from torch.utils.data import Dataset, DataLoader,random_split, SubsetRandomSampler\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from dataset import TrainDataset, TestDataset, img_transform, TrainDatasetAgeAugmentation\n",
    "from avgMeter import AverageMeter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from focal_loss import FocalLoss\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import copy\n",
    "import random\n",
    "import os \n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "equipped-shore",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = img_transform()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dimensional-sculpture",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root = '/opt/ml/input/data/train/images'\n",
    "test_root = '/opt/ml/input/data/eval'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "oriental-activation",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "lr = 0.0001\n",
    "num_epochs = 60\n",
    "model_name = 'effnetb4-batchsize_' + str(batch_size) + '-lr_' + str(lr).split('.')[1] + '-epoch_' + str(num_epochs) + '-CenterCrop' + '-scheduler' \\\n",
    "                + '-nofreeze' + '-fix_data' + '-no_overlap' + '-focal_loss'\n",
    "log_dir = '/opt/ml/code/log/' + model_name + '.txt' \n",
    "save_dir = '/opt/ml/code/trained_models/' + model_name + '.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "capable-windows",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TrainDataset(train_root, input_size = 224, transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "iraqi-failure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'마스크 쓴 데이터의 0.6을 validation으로 옮김.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''마스크 쓴 데이터의 0.6을 validation으로 옮김.'''\n",
    "# indices = list(range(len(data)))\n",
    "# split_idx = int(len(data) * 0.8)\n",
    "# train_idx, valid_idx = [], []\n",
    "# filter_idx = {0,4}\n",
    "# for i in range(split_idx):\n",
    "#     if (i%7) in filter_idx:\n",
    "#         train_idx.append(i)\n",
    "#     else:\n",
    "#         prob = random.uniform(0,1)\n",
    "#         if prob > 0.4:\n",
    "#             valid_idx.append(i)\n",
    "#         else:\n",
    "#             train_idx.append(i)\n",
    "# valid_idx.extend(indices[split_idx:])\n",
    "# print(len(train_idx))\n",
    "# print(len(valid_idx))\n",
    "\n",
    "\n",
    "# train_sampler = SubsetRandomSampler(train_idx)\n",
    "# valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# train_loader = DataLoader(data, batch_size=batch_size, num_workers = 4, sampler=train_sampler, pin_memory=True, shuffle=False)\n",
    "# valid_loader = DataLoader(data, batch_size=batch_size, num_workers = 4, sampler=valid_sampler, pin_memory=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "physical-accreditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(data)))\n",
    "split_idx = int(len(data) * 0.8)\n",
    "train_idx, valid_idx = indices[:split_idx], indices[split_idx:]\n",
    "\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = DataLoader(data, batch_size=batch_size, num_workers = 4, sampler=train_sampler, pin_memory=True, shuffle=False)\n",
    "valid_loader = DataLoader(data, batch_size=batch_size, num_workers = 4, sampler=valid_sampler, pin_memory=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "solved-coaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, val = random_split(data, [int(len(data)*0.8), int(len(data)*0.2)])\n",
    "# train_loader = DataLoader(train, batch_size=batch_size, num_workers = 4,  pin_memory=True, shuffle=True)\n",
    "# valid_loader = DataLoader(val, batch_size=batch_size, num_workers = 4,  pin_memory=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-liberal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "least-degree",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "effnetb4-batchsize_128-lr_0001-epoch_60-CenterCrop-scheduler-nofreeze-fix_data-no_overlap-focal_loss\n"
     ]
    }
   ],
   "source": [
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "driven-clause",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/ml/code/trained_models/effnetb4-batchsize_128-lr_0001-epoch_60-CenterCrop-scheduler-nofreeze-fix_data-no_overlap-focal_loss.pt\n"
     ]
    }
   ],
   "source": [
    "print(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "overall-hebrew",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b4\n"
     ]
    }
   ],
   "source": [
    "model = EfficientNet.from_pretrained('efficientnet-b4', num_classes=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "studied-guitar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haha\n"
     ]
    }
   ],
   "source": [
    "# model._fc = nn.Linear(in_features=1280, out_features=18, bias=True)\n",
    "model.cuda()\n",
    "print('haha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "micro-radio",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n, p in model.named_parameters():\n",
    "#     if '_fc' not in n:\n",
    "#         p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dimensional-latin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = FocalLoss(gamma = 3)\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode = 'min', factor = 0.1, patience = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "grave-variety",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [  1/ 60] | Train Loss 0.8229 | Train Acc 0.7083 | Valid Loss 0.2118 | Valid Acc 0.8561 | f1_score 0.8649\n",
      "epoch [  2/ 60] | Train Loss 0.1106 | Train Acc 0.8989 | Valid Loss 0.1309 | Valid Acc 0.8741 | f1_score 0.8733\n",
      "epoch [  3/ 60] | Train Loss 0.0505 | Train Acc 0.9319 | Valid Loss 0.1158 | Valid Acc 0.8862 | f1_score 0.8868\n",
      "epoch [  4/ 60] | Train Loss 0.0334 | Train Acc 0.9508 | Valid Loss 0.1145 | Valid Acc 0.8899 | f1_score 0.8912\n",
      "epoch [  5/ 60] | Train Loss 0.0197 | Train Acc 0.9687 | Valid Loss 0.1031 | Valid Acc 0.8942 | f1_score 0.8939\n",
      "epoch [  6/ 60] | Train Loss 0.0146 | Train Acc 0.9774 | Valid Loss 0.1146 | Valid Acc 0.8833 | f1_score 0.8806\n",
      "epoch [  7/ 60] | Train Loss 0.0108 | Train Acc 0.9835 | Valid Loss 0.1242 | Valid Acc 0.8910 | f1_score 0.8895\n",
      "epoch [  8/ 60] | Train Loss 0.0082 | Train Acc 0.9873 | Valid Loss 0.1143 | Valid Acc 0.8907 | f1_score 0.8892\n",
      "epoch [  9/ 60] | Train Loss 0.0093 | Train Acc 0.9878 | Valid Loss 0.1236 | Valid Acc 0.8921 | f1_score 0.8918\n",
      "epoch [ 10/ 60] | Train Loss 0.0047 | Train Acc 0.9923 | Valid Loss 0.1256 | Valid Acc 0.8865 | f1_score 0.8848\n",
      "epoch [ 11/ 60] | Train Loss 0.0030 | Train Acc 0.9962 | Valid Loss 0.1293 | Valid Acc 0.8865 | f1_score 0.8831\n",
      "epoch [ 12/ 60] | Train Loss 0.0025 | Train Acc 0.9967 | Valid Loss 0.1267 | Valid Acc 0.8958 | f1_score 0.8933\n",
      "epoch [ 13/ 60] | Train Loss 0.0024 | Train Acc 0.9973 | Valid Loss 0.1257 | Valid Acc 0.8979 | f1_score 0.8957\n",
      "epoch [ 14/ 60] | Train Loss 0.0025 | Train Acc 0.9978 | Valid Loss 0.1258 | Valid Acc 0.8931 | f1_score 0.8907\n",
      "epoch [ 15/ 60] | Train Loss 0.0026 | Train Acc 0.9958 | Valid Loss 0.1260 | Valid Acc 0.8952 | f1_score 0.8931\n",
      "epoch [ 16/ 60] | Train Loss 0.0020 | Train Acc 0.9978 | Valid Loss 0.1335 | Valid Acc 0.8918 | f1_score 0.8886\n",
      "epoch [ 17/ 60] | Train Loss 0.0019 | Train Acc 0.9979 | Valid Loss 0.1231 | Valid Acc 0.8942 | f1_score 0.8926\n",
      "epoch [ 18/ 60] | Train Loss 0.0019 | Train Acc 0.9978 | Valid Loss 0.1312 | Valid Acc 0.8926 | f1_score 0.8893\n",
      "epoch [ 19/ 60] | Train Loss 0.0020 | Train Acc 0.9971 | Valid Loss 0.1309 | Valid Acc 0.8926 | f1_score 0.8908\n",
      "epoch [ 20/ 60] | Train Loss 0.0019 | Train Acc 0.9977 | Valid Loss 0.1259 | Valid Acc 0.8902 | f1_score 0.8869\n",
      "epoch [ 21/ 60] | Train Loss 0.0017 | Train Acc 0.9981 | Valid Loss 0.1274 | Valid Acc 0.8905 | f1_score 0.8884\n",
      "epoch [ 22/ 60] | Train Loss 0.0019 | Train Acc 0.9975 | Valid Loss 0.1242 | Valid Acc 0.8968 | f1_score 0.8951\n",
      "epoch [ 23/ 60] | Train Loss 0.0019 | Train Acc 0.9972 | Valid Loss 0.1238 | Valid Acc 0.8987 | f1_score 0.8975\n",
      "epoch [ 24/ 60] | Train Loss 0.0016 | Train Acc 0.9979 | Valid Loss 0.1183 | Valid Acc 0.9040 | f1_score 0.9029\n",
      "epoch [ 25/ 60] | Train Loss 0.0019 | Train Acc 0.9976 | Valid Loss 0.1245 | Valid Acc 0.8926 | f1_score 0.8905\n",
      "epoch [ 26/ 60] | Train Loss 0.0016 | Train Acc 0.9985 | Valid Loss 0.1296 | Valid Acc 0.8915 | f1_score 0.8886\n",
      "epoch [ 27/ 60] | Train Loss 0.0018 | Train Acc 0.9980 | Valid Loss 0.1290 | Valid Acc 0.8960 | f1_score 0.8939\n",
      "epoch [ 28/ 60] | Train Loss 0.0019 | Train Acc 0.9976 | Valid Loss 0.1269 | Valid Acc 0.8950 | f1_score 0.8926\n",
      "epoch [ 29/ 60] | Train Loss 0.0017 | Train Acc 0.9981 | Valid Loss 0.1307 | Valid Acc 0.8937 | f1_score 0.8907\n",
      "epoch [ 30/ 60] | Train Loss 0.0017 | Train Acc 0.9983 | Valid Loss 0.1290 | Valid Acc 0.8937 | f1_score 0.8908\n",
      "epoch [ 31/ 60] | Train Loss 0.0017 | Train Acc 0.9980 | Valid Loss 0.1285 | Valid Acc 0.8955 | f1_score 0.8918\n",
      "epoch [ 32/ 60] | Train Loss 0.0018 | Train Acc 0.9982 | Valid Loss 0.1261 | Valid Acc 0.8966 | f1_score 0.8953\n",
      "epoch [ 33/ 60] | Train Loss 0.0021 | Train Acc 0.9979 | Valid Loss 0.1272 | Valid Acc 0.8934 | f1_score 0.8916\n",
      "epoch [ 34/ 60] | Train Loss 0.0018 | Train Acc 0.9981 | Valid Loss 0.1380 | Valid Acc 0.8923 | f1_score 0.8902\n",
      "epoch [ 35/ 60] | Train Loss 0.0020 | Train Acc 0.9974 | Valid Loss 0.1240 | Valid Acc 0.8976 | f1_score 0.8958\n",
      "epoch [ 36/ 60] | Train Loss 0.0020 | Train Acc 0.9976 | Valid Loss 0.1292 | Valid Acc 0.8960 | f1_score 0.8941\n",
      "epoch [ 37/ 60] | Train Loss 0.0016 | Train Acc 0.9985 | Valid Loss 0.1267 | Valid Acc 0.8921 | f1_score 0.8896\n",
      "epoch [ 38/ 60] | Train Loss 0.0018 | Train Acc 0.9979 | Valid Loss 0.1272 | Valid Acc 0.8937 | f1_score 0.8911\n",
      "epoch [ 39/ 60] | Train Loss 0.0022 | Train Acc 0.9968 | Valid Loss 0.1288 | Valid Acc 0.8934 | f1_score 0.8901\n",
      "epoch [ 40/ 60] | Train Loss 0.0021 | Train Acc 0.9974 | Valid Loss 0.1318 | Valid Acc 0.8971 | f1_score 0.8940\n",
      "epoch [ 41/ 60] | Train Loss 0.0020 | Train Acc 0.9979 | Valid Loss 0.1327 | Valid Acc 0.8929 | f1_score 0.8903\n",
      "epoch [ 42/ 60] | Train Loss 0.0019 | Train Acc 0.9975 | Valid Loss 0.1362 | Valid Acc 0.8915 | f1_score 0.8879\n",
      "epoch [ 43/ 60] | Train Loss 0.0020 | Train Acc 0.9973 | Valid Loss 0.1345 | Valid Acc 0.8974 | f1_score 0.8958\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-625b5a681b31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mpred_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_logit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_acc = 0.0\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "train_loss, train_acc = AverageMeter(), AverageMeter()\n",
    "valid_loss, valid_acc = AverageMeter(), AverageMeter()\n",
    "f1 = AverageMeter()\n",
    "with open(log_dir, 'w') as log:\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss.reset()\n",
    "        train_acc.reset()\n",
    "        for iter, (img, label) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            img, label = img.float().cuda(), label.cuda()\n",
    "\n",
    "            pred_logit = model(img)\n",
    "\n",
    "            loss = criterion(pred_logit, label)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pred_label = pred_logit.argmax(-1)\n",
    "            acc = (pred_label == label).sum().float() / img.size(0)\n",
    "\n",
    "            train_loss.update(loss.item(), len(img))\n",
    "            train_acc.update(acc, len(img))\n",
    "        \n",
    "            \n",
    "            \n",
    "        valid_loss.reset()\n",
    "        valid_acc.reset()\n",
    "        f1.reset()\n",
    "        for img, label in valid_loader:\n",
    "            img, label = img.float().cuda(), label.cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred_logit = model(img)\n",
    "\n",
    "\n",
    "            loss = criterion(pred_logit, label)\n",
    "\n",
    "            pred_label = pred_logit.argmax(-1)\n",
    "            acc = (pred_label == label).sum().float() / img.size(0)\n",
    "            \n",
    "            valid_loss.update(loss.item(), len(img))\n",
    "            valid_acc.update(acc, len(img))\n",
    "            f1.update(f1_score(pred_label.cpu(), label.cpu(), average='weighted'), len(img))\n",
    "            \n",
    "        train_loss_val = train_loss.avg\n",
    "        train_acc_val = train_acc.avg\n",
    "        valid_loss_val = valid_loss.avg\n",
    "        valid_acc_val = valid_acc.avg\n",
    "        f1_val = f1.avg\n",
    "        \n",
    "        print(\"epoch [%3d/%3d] | Train Loss %.4f | Train Acc %.4f | Valid Loss %.4f | Valid Acc %.4f | f1_score %.4f\" %\n",
    "            (epoch+1, num_epochs, train_loss_val, train_acc_val, valid_loss_val, valid_acc_val, f1_val))\n",
    "        \n",
    "        if valid_acc_val > best_val_acc:\n",
    "            best_val_acc = valid_acc_val\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        scheduler.step(valid_loss_val)\n",
    "        # Train Log Writing\n",
    "        log.write(\"epoch [%3d/%3d] | Train Loss %.4f | Train Acc %.4f | Valid Loss %.4f | Valid Acc %.4f | f1_score %.4f\\n\" %\n",
    "            (epoch+1, num_epochs, train_loss_val, train_acc_val, valid_loss_val, valid_acc_val, f1_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "devoted-journal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EfficientNet'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "union-territory",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(best_model_wts)\n",
    "torch.save(model, save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
