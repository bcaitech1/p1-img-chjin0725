{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
    "from torchvision import transforms\n",
    "from torchvision.models import vgg11\n",
    "from torch.utils.data import Dataset, DataLoader,random_split, SubsetRandomSampler, WeightedRandomSampler\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from focal_loss import FocalLoss\n",
    "from label_smoothing import LabelSmoothingLoss\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from dataset import TrainDataset, TestDataset, img_transform, TrainDatasetAgeAugmentation\n",
    "from avgMeter import AverageMeter\n",
    "from acc_per_label import AccPerLabel\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = img_transform()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root = '/opt/ml/input/data/train/images'\n",
    "test_root = '/opt/ml/input/data/eval'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "lr = 0.0001\n",
    "num_epochs = 15\n",
    "model_name = 'effnetb0-batchsize_' + str(batch_size) + '-lr_' + str(lr).split('.')[1] + '-epoch_' + str(num_epochs)  + '-cosin_warm_restart' \\\n",
    "                + '-nofreeze' + '-fix_data' + '-no_overlap' + '-age_aug58' + '-Hflip05' + '-center_crop500250' + '-LS_alpha06'\n",
    "log_dir = '/opt/ml/code/log/' + model_name + '.txt' \n",
    "save_dir = '/opt/ml/code/trained_models/' + model_name + '.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = TrainDataset(train_root, input_size = 224, transform = transform)\n",
    "data = TrainDatasetAgeAugmentation(train_root, input_size = 224, transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(data)))\n",
    "split_idx = int(len(data) * 0.8)\n",
    "train_idx, valid_idx = indices[:split_idx], indices[split_idx:]\n",
    "\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = DataLoader(data, batch_size=batch_size, num_workers = 3, sampler=train_sampler, pin_memory=True, shuffle=False)\n",
    "valid_loader = DataLoader(data, batch_size=batch_size, num_workers = 3, sampler=valid_sampler, pin_memory=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_class = 18\n",
    "\n",
    "# indices = list(range(len(data)))\n",
    "# split_idx = int(len(data) * 0.8)\n",
    "# train_idx, valid_idx = indices[:split_idx], indices[split_idx:]\n",
    "\n",
    "# ##################\n",
    "# from collections import Counter\n",
    "# labels = data.label_list[:split_idx]\n",
    "# class_weight = Counter(labels)\n",
    "# for i in range(len(class_weight)):\n",
    "#     class_weight[i] = 100.0 / class_weight[i]\n",
    "\n",
    "# class_weight = torch.Tensor([class_weight[i] for i in range(num_class)])\n",
    "# # class_weight = torch.Tensor([1/0.97, 1/0.69, 1/0.67, 1/0.95, 1/0.83, 1/0.51, 1/0.94, 1/0.63, 1/0.76, 1/0.94 \\\n",
    "# #                             , 1/0.8, 1/0.62, 1/0.96, 1/0.71, 1/0.73, 1/0.95, 1/0.8, 1/0.55]) - 0.4\n",
    "\n",
    "# # class_weight = torch.Tensor([1/0.97, 1/0.6, 1/0.67, 1/0.95, 1/0.51, 1/0.51, 1/0.94, 1/0.63, 1/0.76, 1/0.94 \\\n",
    "# #                             , 1/0.6, 1/0.62, 1/0.96, 1/0.6, 1/0.73, 1/0.95, 1/0.6, 1/0.55]) - 0.6\n",
    "# class_weight_all = class_weight[torch.Tensor(labels).long()]\n",
    "\n",
    "# train_sampler = WeightedRandomSampler(\n",
    "#     weights=class_weight_all,\n",
    "#     num_samples=len(class_weight_all),\n",
    "#     replacement=True\n",
    "# )\n",
    "# ######################\n",
    "# valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# train_loader = DataLoader(data, batch_size=batch_size, num_workers = 4, sampler=train_sampler, pin_memory=True, shuffle=False)\n",
    "# valid_loader = DataLoader(data, batch_size=batch_size, num_workers = 4, sampler=valid_sampler, pin_memory=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, val = random_split(data, [int(len(data)*0.8), int(len(data)*0.2)])\n",
    "# train_loader = DataLoader(train, batch_size=batch_size, num_workers = 4,  pin_memory=True, shuffle=True)\n",
    "# valid_loader = DataLoader(val, batch_size=batch_size, num_workers = 4,  pin_memory=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "effnetb0-batchsize_128-lr_0001-epoch_15-cosin_warm_restart-nofreeze-fix_data-no_overlap-age_aug58-Hflip05-center_crop500250-LS_alpha06\n"
     ]
    }
   ],
   "source": [
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/ml/code/trained_models/effnetb0-batchsize_128-lr_0001-epoch_15-cosin_warm_restart-nofreeze-fix_data-no_overlap-age_aug58-Hflip05-center_crop500250-LS_alpha06.pt\n"
     ]
    }
   ],
   "source": [
    "print(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "model = EfficientNet.from_pretrained('efficientnet-b0', num_classes=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haha\n"
     ]
    }
   ],
   "source": [
    "# model._fc = nn.Linear(in_features=1280, out_features=18, bias=True)\n",
    "model.cuda()\n",
    "print('haha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n, p in model.named_parameters():\n",
    "#     if '_fc' not in n:\n",
    "#         p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss()\n",
    "# criterion = FocalLoss(gamma = 100)\n",
    "criterion = LabelSmoothingLoss(classes=18, smoothing=0.6)\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, 5, 1)\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode = 'min', factor = 0.1, patience = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [  1/ 15] | Train Loss 2.6173 | Train Acc 0.6466 | Valid Loss 2.5282 | Valid Acc 0.7831 | f1_score 0.5873\n",
      "class_number num_true_positive/num_class_i = acc\n",
      "0   505.0/515= 0.98|| 1   252.0/390= 0.65|| 2   100.0/165= 0.61|| 3   691.0/730= 0.95|| 4   597.0/690= 0.87|| 5   93.0/210= 0.44|| 6   90.0/103= 0.87|| 7   16.0/78= 0.21|| 8   0.0/33= 0.00|| \n",
      "9   128.0/146= 0.88|| 10   92.0/138= 0.67|| 11   1.0/42= 0.02|| 12   100.0/103= 0.97|| 13   32.0/78= 0.41|| 14   1.0/33= 0.03|| 15   138.0/146= 0.95|| 16   124.0/138= 0.90|| 17   0.0/42= 0.00|| \n",
      "epoch [  2/ 15] | Train Loss 2.4888 | Train Acc 0.8397 | Valid Loss 2.4931 | Valid Acc 0.8328 | f1_score 0.6959\n",
      "class_number num_true_positive/num_class_i = acc\n",
      "0   506.0/515= 0.98|| 1   304.0/390= 0.78|| 2   107.0/165= 0.65|| 3   677.0/730= 0.93|| 4   611.0/690= 0.89|| 5   93.0/210= 0.44|| 6   93.0/103= 0.90|| 7   55.0/78= 0.71|| 8   0.0/33= 0.00|| \n",
      "9   136.0/146= 0.93|| 10   123.0/138= 0.89|| 11   7.0/42= 0.17|| 12   99.0/103= 0.96|| 13   60.0/78= 0.77|| 14   5.0/33= 0.15|| 15   139.0/146= 0.95|| 16   126.0/138= 0.91|| 17   7.0/42= 0.17|| \n",
      "epoch [  3/ 15] | Train Loss 2.4611 | Train Acc 0.8823 | Valid Loss 2.4858 | Valid Acc 0.8331 | f1_score 0.7300\n",
      "class_number num_true_positive/num_class_i = acc\n",
      "0   508.0/515= 0.99|| 1   282.0/390= 0.72|| 2   111.0/165= 0.67|| 3   689.0/730= 0.94|| 4   578.0/690= 0.84|| 5   107.0/210= 0.51|| 6   95.0/103= 0.92|| 7   52.0/78= 0.67|| 8   13.0/33= 0.39|| \n",
      "9   133.0/146= 0.91|| 10   121.0/138= 0.88|| 11   11.0/42= 0.26|| 12   100.0/103= 0.97|| 13   57.0/78= 0.73|| 14   17.0/33= 0.52|| 15   139.0/146= 0.95|| 16   121.0/138= 0.88|| 17   15.0/42= 0.36|| \n",
      "epoch [  4/ 15] | Train Loss 2.4491 | Train Acc 0.9021 | Valid Loss 2.4851 | Valid Acc 0.8354 | f1_score 0.7347\n",
      "class_number num_true_positive/num_class_i = acc\n",
      "0   507.0/515= 0.98|| 1   277.0/390= 0.71|| 2   111.0/165= 0.67|| 3   685.0/730= 0.94|| 4   588.0/690= 0.85|| 5   108.0/210= 0.51|| 6   95.0/103= 0.92|| 7   50.0/78= 0.64|| 8   13.0/33= 0.39|| \n",
      "9   135.0/146= 0.92|| 10   118.0/138= 0.86|| 11   14.0/42= 0.33|| 12   101.0/103= 0.98|| 13   65.0/78= 0.83|| 14   17.0/33= 0.52|| 15   137.0/146= 0.94|| 16   122.0/138= 0.88|| 17   15.0/42= 0.36|| \n",
      "epoch [  5/ 15] | Train Loss 2.4452 | Train Acc 0.9112 | Valid Loss 2.4840 | Valid Acc 0.8399 | f1_score 0.7109\n",
      "class_number num_true_positive/num_class_i = acc\n",
      "0   506.0/515= 0.98|| 1   289.0/390= 0.74|| 2   110.0/165= 0.67|| 3   685.0/730= 0.94|| 4   607.0/690= 0.88|| 5   108.0/210= 0.51|| 6   95.0/103= 0.92|| 7   48.0/78= 0.62|| 8   13.0/33= 0.39|| \n",
      "9   137.0/146= 0.94|| 10   122.0/138= 0.88|| 11   10.0/42= 0.24|| 12   100.0/103= 0.97|| 13   58.0/78= 0.74|| 14   14.0/33= 0.42|| 15   137.0/146= 0.94|| 16   122.0/138= 0.88|| 17   14.0/42= 0.33|| \n",
      "epoch [  6/ 15] | Train Loss 2.4400 | Train Acc 0.9194 | Valid Loss 2.4824 | Valid Acc 0.8368 | f1_score 0.7432\n",
      "class_number num_true_positive/num_class_i = acc\n",
      "0   507.0/515= 0.98|| 1   273.0/390= 0.70|| 2   124.0/165= 0.75|| 3   689.0/730= 0.94|| 4   569.0/690= 0.82|| 5   112.0/210= 0.53|| 6   97.0/103= 0.94|| 7   52.0/78= 0.67|| 8   18.0/33= 0.55|| \n",
      "9   134.0/146= 0.92|| 10   116.0/138= 0.84|| 11   20.0/42= 0.48|| 12   100.0/103= 0.97|| 13   46.0/78= 0.59|| 14   24.0/33= 0.73|| 15   139.0/146= 0.95|| 16   117.0/138= 0.85|| 17   26.0/42= 0.62|| \n",
      "epoch [  7/ 15] | Train Loss 2.4241 | Train Acc 0.9446 | Valid Loss 2.4803 | Valid Acc 0.8423 | f1_score 0.7532\n",
      "class_number num_true_positive/num_class_i = acc\n",
      "0   506.0/515= 0.98|| 1   295.0/390= 0.76|| 2   107.0/165= 0.65|| 3   690.0/730= 0.95|| 4   567.0/690= 0.82|| 5   124.0/210= 0.59|| 6   96.0/103= 0.93|| 7   58.0/78= 0.74|| 8   22.0/33= 0.67|| \n",
      "9   133.0/146= 0.91|| 10   122.0/138= 0.88|| 11   21.0/42= 0.50|| 12   99.0/103= 0.96|| 13   49.0/78= 0.63|| 14   23.0/33= 0.70|| 15   136.0/146= 0.93|| 16   113.0/138= 0.82|| 17   23.0/42= 0.55|| \n",
      "epoch [  8/ 15] | Train Loss 2.4131 | Train Acc 0.9647 | Valid Loss 2.4818 | Valid Acc 0.8362 | f1_score 0.7628\n",
      "class_number num_true_positive/num_class_i = acc\n",
      "0   503.0/515= 0.98|| 1   271.0/390= 0.69|| 2   121.0/165= 0.73|| 3   693.0/730= 0.95|| 4   564.0/690= 0.82|| 5   98.0/210= 0.47|| 6   95.0/103= 0.92|| 7   62.0/78= 0.79|| 8   19.0/33= 0.58|| \n",
      "9   136.0/146= 0.93|| 10   118.0/138= 0.86|| 11   21.0/42= 0.50|| 12   101.0/103= 0.98|| 13   55.0/78= 0.71|| 14   25.0/33= 0.76|| 15   137.0/146= 0.94|| 16   121.0/138= 0.88|| 17   21.0/42= 0.50|| \n",
      "epoch [  9/ 15] | Train Loss 2.4081 | Train Acc 0.9716 | Valid Loss 2.4813 | Valid Acc 0.8405 | f1_score 0.7564\n",
      "class_number num_true_positive/num_class_i = acc\n",
      "0   506.0/515= 0.98|| 1   287.0/390= 0.74|| 2   121.0/165= 0.73|| 3   688.0/730= 0.94|| 4   560.0/690= 0.81|| 5   108.0/210= 0.51|| 6   97.0/103= 0.94|| 7   57.0/78= 0.73|| 8   20.0/33= 0.61|| \n",
      "9   134.0/146= 0.92|| 10   118.0/138= 0.86|| 11   23.0/42= 0.55|| 12   101.0/103= 0.98|| 13   55.0/78= 0.71|| 14   26.0/33= 0.79|| 15   137.0/146= 0.94|| 16   115.0/138= 0.83|| 17   24.0/42= 0.57|| \n",
      "epoch [ 10/ 15] | Train Loss 2.4057 | Train Acc 0.9754 | Valid Loss 2.4803 | Valid Acc 0.8407 | f1_score 0.7627\n",
      "class_number num_true_positive/num_class_i = acc\n",
      "0   504.0/515= 0.98|| 1   285.0/390= 0.73|| 2   112.0/165= 0.68|| 3   689.0/730= 0.94|| 4   560.0/690= 0.81|| 5   116.0/210= 0.55|| 6   96.0/103= 0.93|| 7   58.0/78= 0.74|| 8   22.0/33= 0.67|| \n",
      "9   136.0/146= 0.93|| 10   116.0/138= 0.84|| 11   22.0/42= 0.52|| 12   99.0/103= 0.96|| 13   55.0/78= 0.71|| 14   27.0/33= 0.82|| 15   138.0/146= 0.95|| 16   121.0/138= 0.88|| 17   22.0/42= 0.52|| \n",
      "epoch [ 11/ 15] | Train Loss 2.4064 | Train Acc 0.9717 | Valid Loss 2.4811 | Valid Acc 0.8407 | f1_score 0.7691\n",
      "class_number num_true_positive/num_class_i = acc\n",
      "0   503.0/515= 0.98|| 1   284.0/390= 0.73|| 2   117.0/165= 0.71|| 3   678.0/730= 0.93|| 4   583.0/690= 0.84|| 5   98.0/210= 0.47|| 6   96.0/103= 0.93|| 7   60.0/78= 0.77|| 8   17.0/33= 0.52|| \n",
      "9   139.0/146= 0.95|| 10   117.0/138= 0.85|| 11   25.0/42= 0.60|| 12   102.0/103= 0.99|| 13   61.0/78= 0.78|| 14   24.0/33= 0.73|| 15   137.0/146= 0.94|| 16   113.0/138= 0.82|| 17   24.0/42= 0.57|| \n",
      "epoch [ 12/ 15] | Train Loss 2.3998 | Train Acc 0.9789 | Valid Loss 2.4834 | Valid Acc 0.8381 | f1_score 0.7529\n",
      "class_number num_true_positive/num_class_i = acc\n",
      "0   508.0/515= 0.99|| 1   284.0/390= 0.73|| 2   112.0/165= 0.68|| 3   683.0/730= 0.94|| 4   561.0/690= 0.81|| 5   118.0/210= 0.56|| 6   98.0/103= 0.95|| 7   54.0/78= 0.69|| 8   20.0/33= 0.61|| \n",
      "9   135.0/146= 0.92|| 10   115.0/138= 0.83|| 11   25.0/42= 0.60|| 12   100.0/103= 0.97|| 13   55.0/78= 0.71|| 14   24.0/33= 0.73|| 15   137.0/146= 0.94|| 16   114.0/138= 0.83|| 17   25.0/42= 0.60|| \n",
      "epoch [ 13/ 15] | Train Loss 2.3953 | Train Acc 0.9868 | Valid Loss 2.4808 | Valid Acc 0.8365 | f1_score 0.7638\n",
      "class_number num_true_positive/num_class_i = acc\n",
      "0   506.0/515= 0.98|| 1   275.0/390= 0.71|| 2   106.0/165= 0.64|| 3   692.0/730= 0.95|| 4   560.0/690= 0.81|| 5   108.0/210= 0.51|| 6   99.0/103= 0.96|| 7   61.0/78= 0.78|| 8   20.0/33= 0.61|| \n",
      "9   133.0/146= 0.91|| 10   117.0/138= 0.85|| 11   22.0/42= 0.52|| 12   102.0/103= 0.99|| 13   58.0/78= 0.74|| 14   27.0/33= 0.82|| 15   139.0/146= 0.95|| 16   115.0/138= 0.83|| 17   22.0/42= 0.52|| \n",
      "epoch [ 14/ 15] | Train Loss 2.3919 | Train Acc 0.9906 | Valid Loss 2.4826 | Valid Acc 0.8323 | f1_score 0.7502\n",
      "class_number num_true_positive/num_class_i = acc\n",
      "0   501.0/515= 0.97|| 1   266.0/390= 0.68|| 2   114.0/165= 0.69|| 3   688.0/730= 0.94|| 4   563.0/690= 0.82|| 5   110.0/210= 0.52|| 6   96.0/103= 0.93|| 7   53.0/78= 0.68|| 8   24.0/33= 0.73|| \n",
      "9   136.0/146= 0.93|| 10   116.0/138= 0.84|| 11   22.0/42= 0.52|| 12   101.0/103= 0.98|| 13   60.0/78= 0.77|| 14   24.0/33= 0.73|| 15   137.0/146= 0.94|| 16   111.0/138= 0.80|| 17   24.0/42= 0.57|| \n",
      "epoch [ 15/ 15] | Train Loss 2.3915 | Train Acc 0.9892 | Valid Loss 2.4813 | Valid Acc 0.8373 | f1_score 0.7530\n",
      "class_number num_true_positive/num_class_i = acc\n",
      "0   500.0/515= 0.97|| 1   279.0/390= 0.72|| 2   115.0/165= 0.70|| 3   692.0/730= 0.95|| 4   565.0/690= 0.82|| 5   106.0/210= 0.50|| 6   98.0/103= 0.95|| 7   52.0/78= 0.67|| 8   23.0/33= 0.70|| \n",
      "9   134.0/146= 0.92|| 10   118.0/138= 0.86|| 11   23.0/42= 0.55|| 12   101.0/103= 0.98|| 13   61.0/78= 0.78|| 14   27.0/33= 0.82|| 15   139.0/146= 0.95|| 16   111.0/138= 0.80|| 17   21.0/42= 0.50|| \n",
      "training time : 22.988211103280385\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "best_f1 = 0.0\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "train_loss, train_acc = AverageMeter(), AverageMeter()\n",
    "valid_loss, valid_acc = AverageMeter(), AverageMeter()\n",
    "f1 = AverageMeter()\n",
    "accs = AccPerLabel(num_class = 18)\n",
    "with open(log_dir, 'w') as log:\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss.reset()\n",
    "        train_acc.reset()\n",
    "        for iter, (img, label) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            img, label = img.float().cuda(), label.cuda()\n",
    "\n",
    "            pred_logit = model(img)\n",
    "\n",
    "            loss = criterion(pred_logit, label)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step(epoch + iter/len(train_loader))\n",
    "            \n",
    "            pred_label = pred_logit.argmax(-1)\n",
    "            acc = (pred_label == label).sum().float() / img.size(0)\n",
    "\n",
    "            train_loss.update(loss.item(), len(img))\n",
    "            train_acc.update(acc, len(img))\n",
    "        \n",
    "            \n",
    "            \n",
    "        valid_loss.reset()\n",
    "        valid_acc.reset()\n",
    "        f1.reset()\n",
    "        accs.reset()\n",
    "        for img, label in valid_loader:\n",
    "            img, label = img.float().cuda(), label.cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred_logit = model(img)\n",
    "\n",
    "\n",
    "            loss = criterion(pred_logit, label)\n",
    "\n",
    "            pred_label = pred_logit.argmax(-1)\n",
    "            acc = (pred_label == label).sum().float() / img.size(0)\n",
    "            \n",
    "            valid_loss.update(loss.item(), len(img))\n",
    "            valid_acc.update(acc, len(img))\n",
    "            pred_label = pred_label.cpu().numpy()\n",
    "            label = label.cpu().numpy()\n",
    "            \n",
    "            accs.update(pred_label, label)\n",
    "            f1.update(f1_score(pred_label, label, average='macro'), len(img))\n",
    "            \n",
    "        train_loss_val = train_loss.avg\n",
    "        train_acc_val = train_acc.avg\n",
    "        valid_loss_val = valid_loss.avg\n",
    "        valid_acc_val = valid_acc.avg\n",
    "        f1_val = f1.avg\n",
    "        \n",
    "        print(\"epoch [%3d/%3d] | Train Loss %.4f | Train Acc %.4f | Valid Loss %.4f | Valid Acc %.4f | f1_score %.4f\" %\n",
    "            (epoch+1, num_epochs, train_loss_val, train_acc_val, valid_loss_val, valid_acc_val, f1_val))\n",
    "        \n",
    "        accs_result = accs.show_result()\n",
    "        print(accs_result)\n",
    "        if f1_val > best_f1:\n",
    "            best_f1 = f1_val\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        \n",
    "        # Train Log Writing\n",
    "        log.write(\"epoch [%3d/%3d] | Train Loss %.4f | Train Acc %.4f | Valid Loss %.4f | Valid Acc %.4f | f1_score %.4f\\n\" %\n",
    "            (epoch+1, num_epochs, train_loss_val, train_acc_val, valid_loss_val, valid_acc_val, f1_val))\n",
    "        log.write(accs_result + '\\n')\n",
    "    print(f\"training time : {(time.time() - start)/60}\")    \n",
    "    log.write(f\"training time : {(time.time() - start)/60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EfficientNet'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(best_model_wts)\n",
    "torch.save(model, save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
